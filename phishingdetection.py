# -*- coding: utf-8 -*-
"""phishingDetection.ipynb

https://colab.research.google.com/drive/1KwAibGB-nRddTFkip8mtey9Zxg1SheFg?usp=sharing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KwAibGB-nRddTFkip8mtey9Zxg1SheFg
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
import urllib.request

url = "https://openphish.com/feed.txt"
phishings1 = []
file = urllib.request.urlopen(url)

for line in file:
 decoded_line = line.decode("utf-8")
 phishings1.append(decoded_line[:-2])

phishings = pd.DataFrame(phishings1) 

import pandas as pd
not_phishing = pd.read_csv(r'/content/drive/MyDrive /ENS 491 Phishing detection/not_phishing.txt', header=None)

url_training_data = pd.DataFrame()
url_training_data['urls'] = phishings

not_phishing.columns = ['index', 'urls']
not_phishing = not_phishing.drop(labels='index', axis=1)

textfile = open('/content/drive/MyDrive /ENS 491 Phishing detection/not_phishing.txt')
dict1 = []
dict2 = []
for line in textfile:
  dict2.append((line.split(',')[1])[:-1])
  dict1.append((line.split(',')[1])[:-1])

not_phishing['urls'] = dict2
not_phishing['domain'] = dict1

from pymongo import MongoClient
import pymongo
CONNECTION_STRING = "mongodb+srv://ens491project:ens491project@urlcontent.ktzy1.mongodb.net/urlContents?retryWrites=true&w=majority"
from pymongo import MongoClient
client = MongoClient(CONNECTION_STRING)
dbname = client['urlContents']

TrancoList1 = dbname["TrancoList"]
TrancoList3 = dbname["yeni"]
openPhishContent = dbname["openPhishContent"]

import pandas as pd
TrancoListdf1 = pd.DataFrame(list(TrancoList1.find()))
TrancoListdf3 = pd.DataFrame(list(TrancoList3.find()))
openPhishContent = pd.DataFrame(list(openPhishContent.find()))


from pymongo import MongoClient
import pymongo

CONNECTION_STRING = "mongodb+srv://ahseni_hamza:ahSen*99@cluster0.9yxj29b.mongodb.net/?retryWrites=true&w=majority"
client = MongoClient(CONNECTION_STRING)
dbname = client['urlContents']

TrancoList2 = dbname["yeni"]
TrancoListdf2 = pd.DataFrame(list(TrancoList2.find()))

CONNECTION_STRING = "mongodb://ens492admin:ens492@143.198.147.247:27017/?directConnection=true&serverSelectionTimeoutMS=10000&authSource=admin&appName=mongosh+1.6.1"
client = MongoClient(CONNECTION_STRING)
dbname = client['urlContents']
whoisds = dbname["whoisds"]
TrancoList4 = dbname["openPhishContent"]

TrancoListdf4 = pd.DataFrame(list(TrancoList4.find()))
whoisds = pd.DataFrame(list(whoisds.find()))

TrancoListdf = pd.concat([TrancoListdf1,TrancoListdf2,TrancoListdf3,TrancoListdf4]) #TrancoListdf2,

"""PREPROCESSING"""

options = [500,403,502,503,475,404,429,'403','405','000','503', '400','404','451','520','418']

index = TrancoListdf[TrancoListdf['content'] == "EEK! Unexpected exception in webdriver.get"].index
TrancoListdf.drop(index , inplace=True)
index2 = TrancoListdf[TrancoListdf['content'] == "EEK! TimeoutException"].index
TrancoListdf.drop(index2 , inplace=True)
index3 = TrancoListdf[TrancoListdf['status_code'].isin(options)].index
TrancoListdf.drop(index3 , inplace=True)

index = openPhishContent[openPhishContent['content'] == "EEK! Unexpected exception in webdriver.get"].index
openPhishContent.drop(index , inplace=True)
index2 = openPhishContent[openPhishContent['content'] == "EEK! TimeoutException"].index
openPhishContent.drop(index2 , inplace=True)
index3 = openPhishContent[openPhishContent['status_code'] != 200].index
openPhishContent.drop(index3 , inplace=True)

index4 = []
for index, row in openPhishContent.iterrows():
    if "You don't have permission to access" in row['content'] or "EEK!" in row['content'] or "Not Found" in row['content'] or len(row['content']) < 50 or "Are you a robot?" in row['content'] or "Access Denied:" in row['content'] or "502 Bad Gateway" in row['content']:
      index4.append(index)
    
openPhishContent.drop(index4 , inplace=True)


index5 = []
for index, row in TrancoListdf.iterrows():
    if "EEK!" in row['content'] or "Not Found" in row['content'] or len(row['content']) < 50 or "You don't have permission to access" in row['content'] or "Are you a robot?" in row['content'] or "Access Denied:" in row['content'] or "502 Bad Gateway" in row['content']:
      index5.append(index)
    
TrancoListdf.drop(index5, inplace=True)

openPhishContent.drop_duplicates(inplace=True)
TrancoListdf.drop_duplicates(inplace=True)

# training sets are ready 

#openPhishContent

#TrancoListdf

textfile = open('/content/drive/MyDrive /ENS 491 Phishing detection/suspwords.txt')
susp_words = []

for line in textfile:
  susp_words.append(line[:-1])

pip install dnstwist
import dnstwist

squatting = [] 

def squatting_list (domain1):   # Typo-Squatting/Bitsquatting
  data = dnstwist.run(domain=domain1, registered=True, format='null')
  for i in range(len(data)):
    if data[i]['fuzzer'] == 'bitsquatting' or data[i]['fuzzer'] == 'homoglyph' or data[i]['fuzzer'] == 'hyphenation' or data[i]['fuzzer'] == 'insertion':
      squatting.append(data[i]['domain'])
    if data[i]['fuzzer'] == 'omission' or  data[i]['fuzzer'] == 'repetition' or data[i]['fuzzer'] == 'replacement' or data[i]['fuzzer'] == 'subdomain' :
      squatting.append(data[i]['domain'])
    if data[i]['fuzzer'] == 'transposition' or data[i]['fuzzer'] == 'vowel-swap' or data[i]['fuzzer'] == 'various':
      squatting.append(data[i]['domain'])

# for i in range(50):
#   squatting_list(not_phishing['urls'][i])

# with open('/content/drive/My Drive/ENS 491 Phishing detection/sq_list.txt', 'w') as sq:
#   for word in squatting:
#     sq.write(word)
#     sq.write('\n')

sq_list = pd.read_csv(r'/content/drive/MyDrive /ENS 491 Phishing detection/sq_list.txt', header=None, names=['squatting'])

def squatting(domain):
  for i in sq_list['squatting']:
    if i[:i.rfind('.')] in domain and len(i[:i.rfind('.')]) > 3:
      return True
  return False

# Features
import re

def urls(url):
  return url

def ct_hyphen (url):
  return url.count('-')

def ct_dot (url):
  return url.count('.')

def ct_equal (url):
  return url.count('=')

def ct_digit(url):
  count = 0
  for i in url:
    if i.isdigit():
      count+=1
  return count

def ct_at(url):
  return url.count('@')

def length(url):
  return len(url)

def longer_than_7(url):
  if len(url) > 7:
    return True
  else:
    return False

def mimics(url):       # combosquatting
  url = url.lower()
  if url == 'bt' or url == 't':
      return True
  for i in not_phishing['domain']:
    i = i.split('.')[0]
    if len(i) > 2 :
      if i in url and i != 't':
        return True
  return False

def domain(url):
  a = str(re.findall(r'^(?:http:\/\/|www\.|https:\/\/)([^\/]+)', url))[2:-2]
  if a == '':
    a = url
  return a

def extension(url): 
    b = str(url).split('.')[-1]
    return b

def susp_words_func (url): # purchase, buy, accept, enable, 
  url = url.lower()
  for i in range(len(susp_words)):
    if susp_words[i] in url:
       return True
  return False

import math
from collections import Counter
 
def entropy(s):
    p, lns = Counter(s), float(len(s))
    return( -sum( count/lns * math.log(count/lns, 2) for count in p.values()))

import sklearn.feature_extraction
import numpy as np

alexa_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-4, max_df=1.0)
counts_matrix = alexa_vc.fit_transform(not_phishing['domain'])
alexa_counts = np.log10(counts_matrix.sum(axis=0).getA1())

def alexa_ngram_count(domain):
    alexa_match = alexa_counts * alexa_vc.transform([domain]).T  # Woot vector multiply and transpose Woo Hoo!
    return float(alexa_match)

#ngram_count('free-online-directory')

word_dataframe = pd.read_csv(r'/content/drive/MyDrive /ENS 491 Phishing detection/words.txt', names=['word'], header=None, dtype={'word': np.str}, encoding='utf-8')

word_dataframe = word_dataframe[word_dataframe['word'].map(lambda x: str(x).isalpha())]
word_dataframe = word_dataframe.applymap(lambda x: str(x).strip().lower())
word_dataframe = word_dataframe.dropna()
word_dataframe = word_dataframe.drop_duplicates()

dict_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-5, max_df=1.0)
counts_matrix1 = dict_vc.fit_transform(word_dataframe['word'])
dict_counts = np.log10(counts_matrix1.sum(axis=0).getA1())

def dict_ngram_count(domain):
    dict_match = dict_counts * dict_vc.transform([domain]).T
    return float(dict_match)

urldf = pd.DataFrame(columns= ['domain','hyphen', 'dot', 'at','digit','equal','longer than 7', 'mimics','u_susp words','u_length','extension', 'entropy',
                                   'alexa_ngram_count','dict_ngram_count','squatting','phishing'])


for index, row in TrancoListdf.iterrows():
    urldf.loc[len(urldf.index)] = [domain(row['url']), ct_hyphen(row['url']), ct_dot(row['url']), ct_at(row['url']), ct_digit(row['url']),
                                   ct_equal(row['url']),longer_than_7(row['url']),False,susp_words_func(row['url']),length(row['url']),
                                   extension(row['url']),entropy(row['url']),alexa_ngram_count(row['url']),dict_ngram_count(row['url']),
                                   False,False] 

for index, row in openPhishContent.iterrows():
    urldf.loc[len(urldf.index)] = [domain(row['url']), ct_hyphen(row['url']), ct_dot(row['url']), ct_at(row['url']), ct_digit(row['url']),
                                   ct_equal(row['url']),longer_than_7(row['url']),mimics(row['url']),susp_words_func(row['url']),length(row['url']),
                                   extension(row['url']),entropy(row['url']),alexa_ngram_count(row['url']),dict_ngram_count(row['url']),
                                   squatting(row['url']),True]

# url df is ready   
# content features

def ct_js(text):
  return text.count(".js")

def ct_a_href(text):
  return text.count("<a href")

def ct_a(text):
  return text.count("<a")

def ct_meta(text):
  return text.count("<meta")

def ct_popup(text):
  return text.count("popup") + text.count("Popup")

def ct_iframe(text):
  return text.count("<iframe")

def ct_link(text):
  return text.count("<link")

def ct_link_href(text):
  return text.count("<link href")

def ct_script(text):
  return text.count("<script")

def ct_div(text):
  return text.count("<div")

def ct_ul(text):
  return text.count("<ul") 

def ct_li(text):
  return text.count("<li") 

def ct_php(text):
  return text.count(".php") 

def ct_h(text):
  ct = text.count("<h1") + text.count("<h2") + text.count("<h3") + text.count("<h4") 
  return ct

def ct_img(text):
  return text.count("<img") 

def ct_href(text):
  ct = str(re.findall(r'(href=".*?")', text)).split(',')
  if ct == ['[]']:
    return 0
  return len(ct)

def ct_form(text):
  return text.count("<form action") 

def ct_p(text):
  return text.count("<p") 

def ct_input(text):
  return text.count("<input") 
  
def length(text):
  return len(text)

def totaltags(text):
  return text.count("</")

def ct_susp_words(text): 
  text = text.lower()
  ct = 0
  for i in range(len(susp_words)):
    if susp_words[i] in text:
       ct = ct+1
  return ct

def ct_aspx(text):
  return text.count(".aspx")

def ct_embed(text):
  return text.count("<embed")

def ct_button(text):
  return text.count("<button")

def ct_label(text):
  return text.count("<label")

def ct_input_password(text):
  return text.count("type=\"password\"")

def ct_input_button(text):
  return text.count("type=\"button\"")

def ct_input_checkbox(text):
  return text.count("type=\"checkbox\">")

def ct_input_date(text):
  return text.count("type=\"date\"")

def ct_input_email(text):
  return text.count("type=\"email\"")

def ct_input_image(text):
  return text.count("type=\"image\"")

def ct_input_submit(text):
  return text.count("type=\"submit\"")

def ct_input_text(text):
  return text.count("type=\"text\"")

def ct_input_tel(text):
  return text.count("type=\"tel\"")

def ct_input_radio(text):
  return text.count("type=\"radio\"")

def ct_input_reset(text):
  return text.count("type=\"reset\"")

def ct_input_search(text):
  return text.count("type=\"search\"")

def ct_display_none(text):
  return text.count("display: none;")

def ct_visibility_none(text):
  return text.count("visibility: hidden;")

def ct_script_type_js(text):
  return text.count("<script type=\"text/javascript\"")

def ct_svg(text):
  return text.count("</svg>")

def ct_facebook(text):
  return text.lower().count("facebook")

def ct_twitter(text):
  return text.lower().count("twitter")

def ct_instagram(text):
  return text.lower().count("instagram")

def ct_youtube(text):
  return text.lower().count("youtube")

def ct_pinterest(text):
  return text.lower().count("pinterest")

def ct_linkedin(text):
  return text.lower().count("linkedin")

def ct_hidden(text):
  return text.count('hidden') + text.count(".hidden") + text.count("#hidden") + text.count("\"hidden") + text.count("*[visibility=\"none\"]") + text.count("*[display=\"none\"]")

def ct_empty_href(text):
  return text.count("href=\"#") + text.count("href=\"#skip\"") + text.count("href=\"#content\"") + text.count("href=\"javascript:;") + text.count("href=\"javascript::void(0);") + text.count("href=\"javascript:void(0);") + text.count("href=\"\\\"") + text.count("href=''")

def ct_intlink(text, domain1):
  counter = 0
  if ct_href(text) == 0:
    return 0
  a = str(re.findall(r'(href=".*?")', text))
  a = a.split(',')
  b = []
  for i in range(len(a)):
    link = str(re.findall(r'(".*?")', a[i]))
    link = link[3:-3]
    b.append(link)
  for i in range(len(b)):
    if domain(b[i]) == domain1 or domain(b[i]) == '':
      counter = counter+1
  return counter

def ct_extlink(text,domain1):
   return ct_href(text) - ct_intlink(text,domain1)

def ct_addEvent(text):
  return text.count('addEventListener')

def ct_source(text):
  return text.count("<source")

def ct_blockquote(text):
  return text.count("<blockquote")

def ct_hasAttribute(text):
  return text.count('hasAttribute') + text.count('hasAttributes')

def ct_getEntriesByName(text):
  return text.count('getEntriesByName')

def ct_media(text):
  return text.count('@media')

def ct_accessKey(text):
  return text.count('accessKey')

def ct_click(text):
  return text.count('click(')

def ct_getAttribute(text):
  return text.count('getAttribute(')

def ct_innerHTML(text):
  return text.count('innerHTML')

def ct_innerText(text):
  return text.count("innerText")

def ct_removeAttribute(text):
  return text.count("removeAttribute")

def ct_removeEventListener(text):
  return text.count("removeEventListener")

def ct_setAttribute(text):
  return text.count("setAttribute")

def ct_querySelectorAll(text):
  return text.count("querySelectorAll") + text.count("querySelector")

def ct_freeze(text):
  return text.count("freeze")

def ct_throwError(text):
  return text.count("throw Error")

def ct_call(text):
  return text.count("call(")

contentdf = pd.DataFrame(columns= ['domain','intlink', 'extlink', '.js','a href','a','meta', 'popup','iframe','link','button', 'label', 'emptyhref',
                                   'link href','script','div','ul','li','.php','h','img','href','form','p',
                                   'input','len','tags','suspicious_words', 'input_pass', 'input_button', 'input_checkbox', 'input_date',
                                   'input_email', 'input_image', 'input_submit', 'input_text', 'input_tel', 'input_radio', 'input_reset', 'input_search',
                                   'display_none', 'visibility_none', 'script_type_js', 'svg', 'social media', 'hidden', 'embed','aspx', 'addEventListener', 
                                   'source', 'blockquote', 'hasAttribute', 'getEntriesByName', 'media', 'accesskey', 'click', 'getAttribute', 
                                   'innerHTML', 'innerText', 'removeAttribute', 'removeEventListener', 'setAttribute', 'querySelectorAll', 'freeze', 'throwError', 
                                   'call', 'phishing'])


for index, row in TrancoListdf.iterrows():
    contentdf.loc[len(contentdf.index)] = [domain(row['url']), ct_intlink(row['content'],domain(row['url'])), ct_extlink(row['content'],domain(row['url'])), 
                                           ct_js(row['content']), ct_a_href(row['content']),ct_a(row['content']),ct_meta(row['content']),ct_popup(row['content']),
                                           ct_iframe(row['content']),ct_link(row['content']),ct_button(row['content']),ct_label(row['content']),
                                           ct_empty_href(row['content']),ct_link_href(row['content']),ct_script(row['content']),ct_div(row['content']),ct_ul(row['content']),
                                           ct_li(row['content']),ct_php(row['content']),ct_h(row['content']),ct_img(row['content']),
                                           ct_href(row['content']),ct_form(row['content']),ct_p(row['content']),ct_input(row['content']),length(row['content']),
                                           totaltags(row['content']),ct_susp_words(row['content']), ct_input_password(row['content']), ct_input_button(row['content']),
                                           ct_input_checkbox(row['content']),ct_input_date(row['content']),ct_input_email(row['content']),ct_input_image(row['content']), 
                                           ct_input_submit(row['content']), ct_input_text(row['content']), ct_input_tel(row['content']), ct_input_radio(row['content']),
                                           ct_input_reset(row['content']), ct_input_search(row['content']), ct_display_none(row['content']), ct_visibility_none(row['content']),
                                           ct_script_type_js(row['content']), ct_svg(row['content']), ct_facebook(row['content']) + ct_twitter(row['content']) +
                                           ct_instagram(row['content']) + ct_youtube(row['content']) + ct_pinterest(row['content']) + ct_linkedin(row['content']),
                                           ct_hidden(row['content']), ct_embed(row['content']), ct_aspx(row['content']), ct_addEvent(row['content']), ct_source(row['content']),
                                           ct_blockquote(row['content']), ct_hasAttribute(row['content']),ct_getEntriesByName(row['content']), ct_media(row['content']), 
                                           ct_accessKey(row['content']), ct_click(row['content']), ct_getAttribute(row['content']), ct_innerHTML(row['content']), 
                                           ct_innerText(row['content']), ct_removeAttribute(row['content']), ct_removeEventListener(row['content']), 
                                           ct_setAttribute(row['content']), ct_querySelectorAll(row['content']), ct_freeze(row['content']), ct_throwError(row['content']), 
                                           ct_call(row['content']),False] 

for index, row in openPhishContent.iterrows():
    contentdf.loc[len(contentdf.index)] = [domain(row['url']), ct_intlink(row['content'],domain(row['url'])), ct_extlink(row['content'],domain(row['url'])), 
                                           ct_js(row['content']), ct_a_href(row['content']),ct_a(row['content']),ct_meta(row['content']),ct_popup(row['content']),
                                           ct_iframe(row['content']),ct_link(row['content']),ct_button(row['content']),ct_label(row['content']),
                                           ct_empty_href(row['content']),ct_link_href(row['content']),ct_script(row['content']),ct_div(row['content']),ct_ul(row['content']),
                                           ct_li(row['content']),ct_php(row['content']),ct_h(row['content']),ct_img(row['content']),
                                           ct_href(row['content']),ct_form(row['content']),ct_p(row['content']),ct_input(row['content']),length(row['content']),
                                           totaltags(row['content']),ct_susp_words(row['content']), ct_input_password(row['content']), ct_input_button(row['content']),
                                           ct_input_checkbox(row['content']),ct_input_date(row['content']),ct_input_email(row['content']),ct_input_image(row['content']), 
                                           ct_input_submit(row['content']), ct_input_text(row['content']), ct_input_tel(row['content']), ct_input_radio(row['content']),
                                           ct_input_reset(row['content']), ct_input_search(row['content']), ct_display_none(row['content']), ct_visibility_none(row['content']),
                                           ct_script_type_js(row['content']), ct_svg(row['content']), ct_facebook(row['content']) + ct_twitter(row['content']) +
                                           ct_instagram(row['content']) + ct_youtube(row['content']) + ct_pinterest(row['content']) + ct_linkedin(row['content']),
                                           ct_hidden(row['content']), ct_embed(row['content']), ct_aspx(row['content']), ct_addEvent(row['content']), ct_source(row['content']),
                                           ct_blockquote(row['content']), ct_hasAttribute(row['content']),ct_getEntriesByName(row['content']), ct_media(row['content']), 
                                           ct_accessKey(row['content']), ct_click(row['content']), ct_getAttribute(row['content']), ct_innerHTML(row['content']), 
                                           ct_innerText(row['content']), ct_removeAttribute(row['content']), ct_removeEventListener(row['content']), 
                                           ct_setAttribute(row['content']), ct_querySelectorAll(row['content']), ct_freeze(row['content']), ct_throwError(row['content']), 
                                           ct_call(row['content']),True] 
#content df is ready

training_df = pd.merge(urldf, contentdf, left_index=True, right_index=True)
training_df = training_df.drop('domain_y',axis='columns')
training_df = training_df.rename(columns={'domain_x': 'domain'})
training_df = training_df.drop('phishing_x',axis='columns')
training_df = training_df.rename(columns={'phishing_y': 'phishing'})

"""DATA PREPROCESSING"""

training_df.isnull().values.any()  # No missing values
training_df.dtypes

# Categorical : longer than 7 - squatting - phishing - mimics - u_susp words - extension

from sklearn.preprocessing import OrdinalEncoder

ord_enc = OrdinalEncoder()
training_df["longer than 7"] = ord_enc.fit_transform(training_df[["longer than 7"]])
training_df["squatting"] = ord_enc.fit_transform(training_df[["squatting"]])
training_df["u_susp words"] = ord_enc.fit_transform(training_df[["u_susp words"]])
training_df["mimics"] = ord_enc.fit_transform(training_df[["mimics"]])
training_df["phishing"] = ord_enc.fit_transform(training_df[["phishing"]])

# Removing extensions that are integer

print(len(training_df['extension'].unique()))

for index, row in training_df.iterrows():
  if row['extension'].isnumeric() :
    training_df['extension'][index] = '0'

training_df["extension"] = ord_enc.fit_transform(training_df[["extension"]])
print(len(training_df['extension'].unique()))

# object -> float conversion

domains = training_df['domain']
training_df = training_df.loc[:, training_df.columns != 'domain'].astype(float)

# min-max scalar

# from sklearn.preprocessing import MinMaxScaler
# sc = MinMaxScaler()
# training_df1 = sc.fit_transform(training_df.loc[:, training_df.columns != 'domain'])

training_df.columns

"""STATS"""

training_df_p = training_df[training_df['phishing'] == 1.0]
training_df_b = training_df[training_df['phishing'] == 0.0]

sump_1 = []
sumb_1 = []
cols_1 = []

sump_2 = []
sumb_2 = []
cols_2 = []

sump_3 = []
sumb_3 = []
cols_3 = []

sump_4 = []
sumb_4 = []
cols_4 = []

sump_5 = []
sumb_5 = []
cols_5 = []

sump_6 = []
sumb_6 = []
cols_6 = []

for col in training_df.columns:
  if col != 'len' and col != 'tags':
    if training_df_b[col].sum() >= 10000:
      sump_1.append(training_df_p[col].sum())
      sumb_1.append(training_df_b[col].sum())
      cols_1.append(col)
    elif training_df_b[col].sum() < 10000 and training_df_b[col].sum() >= 5000:
      sump_2.append(training_df_p[col].sum())
      sumb_2.append(training_df_b[col].sum())
      cols_2.append(col)   
    elif training_df_b[col].sum() < 5000 and training_df_b[col].sum() >= 1000:
      sump_3.append(training_df_p[col].sum())
      sumb_3.append(training_df_b[col].sum())
      cols_3.append(col)   
    elif training_df_b[col].sum() < 1000 and training_df_b[col].sum() >= 100:
      sump_4.append(training_df_p[col].sum())
      sumb_4.append(training_df_b[col].sum())
      cols_4.append(col)   
    elif (col == 'hyphen' or col == 'squatting' or col == 'mimics') or (training_df_b[col].sum() < 100 and training_df_b[col].sum() >= 10):
      sump_5.append(training_df_p[col].sum())
      sumb_5.append(training_df_b[col].sum())
      cols_5.append(col)   
    elif training_df_b[col].sum() < 10:
      sump_6.append(training_df_p[col].sum())
      sumb_6.append(training_df_b[col].sum())
      cols_6.append(col)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20, 5))

ax.plot(cols_1,sump_1, color="green",marker = 'o')
ax.plot(cols_1,sumb_1, color="red",marker = 'o')
for tick in ax.get_xticklabels():
    tick.set_rotation(45)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(15,5))

ax.plot(cols_2,sump_2, color="green",marker = 'o')
ax.plot(cols_2,sumb_2, color="red",marker = 'o')
for tick in ax.get_xticklabels():
    tick.set_rotation(45)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20, 5))

ax.plot(cols_3,sump_3, color="green",marker = 'o')
ax.plot(cols_3,sumb_3, color="red",marker = 'o')
for tick in ax.get_xticklabels():
    tick.set_rotation(45)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20, 5))

ax.plot(cols_4,sump_4, color="green",marker = 'o')
ax.plot(cols_4,sumb_4, color="red",marker = 'o')
for tick in ax.get_xticklabels():
    tick.set_rotation(45)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(15, 5))

ax.plot(cols_5,sump_5, color="green",marker = 'o')
ax.plot(cols_5,sumb_5, color="red",marker = 'o')
for tick in ax.get_xticklabels():
    tick.set_rotation(45)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20, 5))

ax.plot(cols_6[:-1],sump_6[:-1], color="green",marker = 'o')
ax.plot(cols_6[:-1],sumb_6[:-1], color="red",marker = 'o')
for tick in ax.get_xticklabels():
    tick.set_rotation(45)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10, 3))

ax.plot('len',training_df_p['len'].sum(), color="green",marker = 'o')
ax.plot('len',training_df_b['len'].sum(), color="red",marker = 'o')
for tick in ax.get_xticklabels():
    tick.set_rotation(45)   

# y = 9 -> 9*1e8 = 9e8 = 9 * 10^8 = 900,000,000.

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10, 3))

ax.plot('tags',training_df_p['tags'].sum(), color="green",marker = 'o')
ax.plot('tags',training_df_b['tags'].sum(), color="red",marker = 'o')
for tick in ax.get_xticklabels():
    tick.set_rotation(45)

stats = pd.DataFrame(columns = ['feature', 'p value', 'b value', 'ratio', 'difference'])
training_df = training_df.astype(int)

for col in training_df.columns:
  if training_df_p[col].sum() > training_df_b[col].sum():
    stats.loc[len(stats.index)] = (col, training_df_p[col].sum(), training_df_b[col].sum(),training_df_p[col].sum()/training_df_b[col].sum(), training_df_p[col].sum()-training_df_b[col].sum() )
  else:
    stats.loc[len(stats.index)] = (col, training_df_p[col].sum(), training_df_b[col].sum(),training_df_b[col].sum()/training_df_p[col].sum(), training_df_b[col].sum()-training_df_p[col].sum() )

pd.set_option('display.max_rows', None)
pd.set_option('display.float_format', lambda x: '%.1f' % x)
stats.sort_values("ratio",ascending=False)

"""Unimportant : input_reset - innerText - embed - input_text - accessKey - at - equal - input_checkbox - input_search """

training_df2 = training_df.drop(['input_reset','innerText','embed','input_text','accesskey','at','equal','input_checkbox','input_search'], axis=1)

"""Feature Extraction"""

hist = training_df2['tags'].hist(bins=100, grid=False, figsize=(25,3), range=[0, 500])
training_df2['tags'].median()

def tags2(value):
    if value < 100:
        return 1
    if 100 <= value < 500:
        return 2
    else:
        return 3
 
training_df2['tags_ext'] = training_df2['tags'].map(tags2)

hist = training_df2['len'].hist(bins=200, grid=False, figsize=(15,3), range=[0, 290000])
training_df2['len'].median()

def len2(value):
    if value < 30000:
        return 1
    if 30000 <= value < 100000:
        return 2
    else:
        return 3
 
training_df2['len_ext'] = training_df2['len'].map(len2)

hist = training_df2['extlink'].hist(bins=100, grid=False, figsize=(15,3))
len(training_df2[training_df2['extlink'] == 0])

def len2(value):
    if value < 25:
        return 1
    if 25 <= value < 200:
        return 2
    else:
        return 3
 
training_df2['len_ext'] = training_df2['len'].map(len2)

hist = training_df2['u_length'].hist(bins=30, grid=False, figsize=(10,3))
training_df2['u_length'].median()

def u_length2(value):
    if value < 15:
        return 1
    if 15 <= value < 40:
        return 2
    else:
        return 3
 
training_df2['u_length_ext'] = training_df2['u_length'].map(u_length2)

hist = training_df2['digit'].hist(bins=20, grid=False, figsize=(10,3))
training_df2['digit'].median()

def digit2(value):
    if value == 0:
        return 1
    else:
        return 3
 
training_df2['digit_ext'] = training_df2['digit'].map(digit2)

hist = training_df2['dot'].hist(bins=5, grid=False, figsize=(10,3))
training_df2['dot'].median()

def dot2(value):
    if value == 1:
        return 1
    else:
        return 3
 
training_df2['dot_ext'] = training_df2['dot'].map(dot2)

hist = training_df2['setAttribute'].hist(bins=5, grid=False, figsize=(10,3))
len(training_df2[training_df2['setAttribute'] == 0])

def setAttribute2(value):
    if value == 0:
        return 1
    else:
        return 3
 
training_df2['setAttribute_ext'] = training_df2['setAttribute'].map(setAttribute2)

hist = training_df2['removeEventListener'].hist(bins=5, grid=False, figsize=(10,3))
len(training_df2[training_df2['removeEventListener'] == 0])

def removeEventListener2(value):
    if value == 0:
        return 1
    else:
        return 3
 
training_df2['removeEventListener_ext'] = training_df2['removeEventListener'].map(removeEventListener2)

hist = training_df2['hyphen'].hist(bins=5, grid=False, figsize=(10,3))
len(training_df2[training_df2['hyphen'] == 0])

def hyphen2(value):
    if value == 0:
        return 1
    else:
        return 3
 
training_df2['hyphen_ext'] = training_df2['hyphen'].map(hyphen2)

hist = training_df2['input_pass'].hist(bins=5, grid=False, figsize=(10,3))
len(training_df2[training_df2['input_pass'] == 0])

def input_pass2(value):
    if value == 0:
        return 1
    else:
        return 3
 
training_df2['input_pass_ext'] = training_df2['input_pass'].map(input_pass2)

hist = training_df2['input_date'].hist(bins=5, grid=False, figsize=(10,3))
len(training_df2[training_df2['input_date'] == 0])

def input_date2(value):
    if value == 0:
        return 1
    else:
        return 3
 
training_df2['input_date_ext'] = training_df2['input_date'].map(input_date2)

training_df2.isnull().values.any() # no missing value

training_df2.corr().style.background_gradient(cmap = 'coolwarm')

def corrFilter(x: pd.DataFrame, bound: float):
    xCorr = x.corr()
    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.0)]
    xFlattened = xFiltered.unstack().sort_values(ascending=False).drop_duplicates()
    return xFlattened

corrFilter(training_df2, .4)

"""Phishing Correlations"""

from tabulate import tabulate

table = [["u_length_ext",0.6], 
         ['li',-0.4], 
         ['a',-0.4], 
         ['tags',-0.4],
         ['meta',-0.5], 
         ['href',-0.5], 
         ['extlink',-0.5], 
         ['susp_words',-0.5], 
         ['entropy',0.4],
         ['dot',0.5]]
print(tabulate(table))

"""Column Decisions Before Random Forest"""

training_df3 = training_df2.drop(['getEntriesByName', 'input_image', 'input_date_ext', 'blockquote', 'removeAttribute', 'input_date', 'input_radio',
                                 'input_tel', 'hasAttribute', 'freeze', 'longer than 7', 'aspx', 'throwError', 'removeEventListener_ext','len_ext','source','getAttribute',
                                  'innerHTML', 'input_email', 'visibility_none'], axis=1)

"""Random Forest"""

from sklearn.utils import shuffle
training_df3 = shuffle(training_df3)

phishing_t = training_df3['phishing']
training_df_t = training_df3.loc[:, training_df3.columns != 'phishing']
training_df_t = training_df3.drop(columns=['digit_ext','display_none','input_submit','media','popup','addEventListener','script_type_js','input_pass', 'form','input_button','svg','label','querySelectorAll',
                                                             'call','click','removeEventListener','input_pass_ext','u_susp words','button', 'social media','tags_ext','hidden','input','ul','a href'])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(training_df_t, phishing_t, train_size=0.8, random_state=42)
X_train.shape, X_test.shape

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42, n_jobs=-1)
# params = {
#     'max_depth': [2,3,5,10,20],
#     'min_samples_leaf': [5,10,20,50,100,200],
#     'n_estimators': [10,25,30,50,100,200]
# }
# from sklearn.model_selection import GridSearchCV
# # Instantiate the grid search model
# grid_search = GridSearchCV(estimator=rf,
#                            param_grid=params,
#                            cv = 4,
#                            n_jobs=-1, verbose=1, scoring="accuracy")

# grid_search.fit(X_train, y_train)

# print(grid_search.best_score_)

# rf_best = grid_search.best_estimator_
# print(rf_best)

# pd.options.display.float_format = '{:,.6f}'.format

# print(rf_best.feature_importances_)

# imp_df = pd.DataFrame({
#     "Varname": X_train.columns,
#     "Imp": rf_best.feature_importances_
# })

# imp_df.sort_values(by="Imp", ascending=False)

classifier_rf = RandomForestClassifier(max_depth=5, min_samples_leaf=5, n_estimators=30,
                       n_jobs=-1, random_state=42,oob_score=True)

classifier_rf.fit(X_train, y_train)
y_pred=classifier_rf.predict(X_test)

from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import ConfusionMatrixDisplay

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

feature_imp = pd.Series(classifier_rf.feature_importances_,index=X_train.columns).sort_values(ascending=False)
print(feature_imp)

cm = confusion_matrix(y_test, y_pred, labels=classifier_rf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=classifier_rf.classes_)
disp.plot(colorbar=False)
plt.show()

print(f1_score(y_test, y_pred, average=None))
print(mean_squared_error(y_test,y_pred))

"""RULES"""

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

#rf_results = pd.DataFrame(columns=['feature','value','gini','samples', 'class'])

def func(ind):
  fig = plt.figure(figsize=(35, 30))
  tree = plot_tree(classifier_rf.estimators_[ind], 
            feature_names=X_train.columns,
            class_names=['True','False'], 
            filled=True, impurity=True, 
            rounded=True)

  # textfile = open('/content/drive/MyDrive /ENS 491 Phishing detection/rftree.txt')

  for line in tree:
    global rf_results
    line = str(line)
    feature = str(re.findall(r'.*, \'([a-zA-Z._0-9\s]+) <= .*', line))[2:-2]
    value = str(re.findall(r'.*, \'[a-zA-Z._0-9\s]+ (<= [0-9.]*)\\n.*', line))[2:-2]
    gini = str(re.findall(r'.*, \'[a-zA-Z._0-9\s]+<= [0-9.]*\\ngini = ([0-9.]*).*', line))[2:-2]
    samples = str(re.findall(r'.*, \'[a-zA-Z._0-9\s]+<= [0-9.]*\\ngini = [0-9.]*\\nsamples = ([0-9]+)\\n.*', line))[2:-2]
    _class = str(re.findall(r'.*, \'[a-zA-Z._0-9\s]+<= [0-9.]*\\ngini = [0-9.]*.*\\nclass = (True|False).*', line))[2:-2]
    if feature != '':
      rf_results = rf_results.append({'feature': feature,'value': value, 'gini': gini,'samples':samples,'class':_class}, ignore_index=True)

  rf_results.sort_values(by="feature", ascending=True)

# for i in range(len(classifier_rf.estimators_)):
#   func(i)

from sklearn.tree import _tree
from google.colab import files
import os

rules_all = pd.DataFrame(columns=['rules','class','probability','samples'])

def get_rules(tree, feature_names, class_names):
    tree_ = tree.tree_
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree_.feature
    ]

    paths = []
    path = []
    
    def recurse(node, path, paths):
        
        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            p1, p2 = list(path), list(path)
            p1 += [f"({name} <= {np.round(threshold, 3)})"]
            recurse(tree_.children_left[node], p1, paths)
            p2 += [f"({name} > {np.round(threshold, 3)})"]
            recurse(tree_.children_right[node], p2, paths)
        else:
            path += [(tree_.value[node], tree_.n_node_samples[node])]
            paths += [path]
            
    recurse(0, path, paths)

    # sort by samples count
    samples_count = [p[-1][1] for p in paths]
    ii = list(np.argsort(samples_count))
    paths = [paths[i] for i in reversed(ii)]
    
    rules = []
    for path in paths:
        rule = "if "
        
        for p in path[:-1]:
            if rule != "if ":
                rule += " and "
            rule += str(p)
        rule += " then "
        if class_names is None:
            rule += "response: "+str(np.round(path[-1][0][0][0],3))
        else:
            classes = path[-1][0][0]
            l = np.argmax(classes)
            rule += f"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)"
        rule += f" | based on {path[-1][1]:,} samples"
        rules += [rule]
        
    return rules

# for i in classifier_rf.estimators_:
#     rules1 = get_rules(i, X_train.columns.to_list(), ['True','False'])
#     for r in rules1:
#         line = str(r)
#         _rules = str(re.findall(r'if (.*) then .*', line))[2:-2]
#         _class = str(re.findall(r'.* then class: (.*) \(proba: .*', line))[2:-2]
#         probability = float(str(re.findall(r'.* \(proba: (.*)%\) .*', line))[2:-2])
#         samples = int(str(re.findall(r'.* based on (.*) samples', line))[2:-2])
#         rules_all = rules_all.append({'rules': _rules,'class': _class, 'probability': probability,'samples':samples}, ignore_index=True)

# rules_all.to_excel("rules_all.xlsx")
# files.download("/content/rules_all.xlsx")

from sklearn.tree import DecisionTreeClassifier 
from sklearn import tree

text_representation = tree.export_text(classifier_rf.estimators_[1], feature_names=X_train.columns.to_list())
print(text_representation)

from sklearn.tree import _tree

class_prob_fin = pd.DataFrame(columns=['rule','class','sample'])

def func(ind):
    def get_rules(tree, feature_names, class_names):
        tree_ = tree.tree_
        feature_name = [
            feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
            for i in tree_.feature
        ]

        paths = []
        path = []
        
        def recurse(node, path, paths):
            
            if tree_.feature[node] != _tree.TREE_UNDEFINED:
                name = feature_name[node]
                threshold = tree_.threshold[node]
                p1, p2 = list(path), list(path)
                p1 += [f"({name} <= {np.round(threshold, 3)})"]
                recurse(tree_.children_left[node], p1, paths)
                p2 += [f"({name} > {np.round(threshold, 3)})"]
                recurse(tree_.children_right[node], p2, paths)
            else:
                path += [(tree_.value[node], tree_.n_node_samples[node])]
                paths += [path]
                
        recurse(0, path, paths)

        # sort by samples count
        samples_count = [p[-1][1] for p in paths]
        ii = list(np.argsort(samples_count))
        paths = [paths[i] for i in reversed(ii)]
        
        rules = []
        for path in paths:
            rule = "if "
            
            for p in path[:-1]:
                if rule != "if ":
                    rule += " and "
                rule += str(p)
            rule += " then "
            if class_names is None:
                rule += "response: "+str(np.round(path[-1][0][0][0],3))
            else:
                classes = path[-1][0][0]
                l = np.argmax(classes)
                rule += f"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)"
            rule += f" | based on {path[-1][1]:,} samples"
            rules += [rule]
            
        return rules

    rules = get_rules(classifier_rf.estimators_[ind], feature_names=X_train.columns.to_list(), class_names=[1.0, 0.0])

    rules1 = pd.DataFrame(columns=['rule','class','probability','sample'])

    for line in rules:
      rule = str(re.findall(r'if\s([a-zA-Z.0-9\s()><=_]+)then.*', line))[2:-2]
      class_ = str(re.findall(r'.*class: ([0-1]{1}).*', line))[2:-2]
      prob = str(re.findall(r'.*\(proba: ([0-9.]+%)\).*', line))[2:-2]
      sample = str(re.findall(r'.*based on ([0-9]+) samples', line))[2:-2]
      rules1 = rules1.append({'rule': rule,'class': float(class_), 'probability': prob,'sample':float(sample)}, ignore_index=True)

    from statistics import mean
    pd.set_option('display.float_format', lambda x: '%.2f' % x)

    class_prob = pd.DataFrame(columns=['rule','class','sample'])

    cond = []
    for index, row in rules1.iterrows():
      rule2 = row['rule'].split('and')
      for i in rule2:
        if i != ',':
          if i[2:-2] not in cond:
            cond.append(i[2:-2])

    for i in cond:
      class_ = []
      count = 0
      for index, row in rules1.iterrows():
        if i in row['rule']:
          class_.append(row['class'])
          count+=1
      class_prob = class_prob.append({'rule': i,'class': mean(class_),'sample':count}, ignore_index=True)
      
    class_prob = class_prob.sort_values(by="sample", ascending=False)

    global class_prob_fin

    for index, row in class_prob.iterrows():
      if index < 30:
        class_prob_fin = class_prob_fin.append({'rule': row['rule'],'class': row['class'],'sample':row['sample']}, ignore_index=True)

#class_prob.to_excel("class_prob"+str(ind)+".xlsx")
# for i in range(len(classifier_rf.estimators_)):
#   func(i)

from pymongo import MongoClient
import pymongo
import pandas as pd

CONNECTION_STRING = "mongodb://ens492admin:ens492@143.198.147.247:27017/?directConnection=true&serverSelectionTimeoutMS=2000&authSource=admin&appName=mongosh+1.6.1"
client = MongoClient(CONNECTION_STRING)
dbname = client['urlContents']
whoisds = dbname["whoisds"]

whoisds = pd.DataFrame(list(whoisds.find()))

whoisds

"""Preprocessing on Test Set"""

training_df_t.columns

options = [500,403,502,503,475,404,429,'403','405','000','503', '400','404','451','520','418']

index = whoisds[whoisds['content'] == "EEK! Unexpected exception in webdriver.get"].index
whoisds.drop(index , inplace=True)
index2 = whoisds[whoisds['content'] == "EEK! TimeoutException"].index
whoisds.drop(index2 , inplace=True)
index3 = whoisds[whoisds['status_code'].isin(options)].index
whoisds.drop(index3 , inplace=True)

index4 = []
for index, row in whoisds.iterrows():
    if "You don't have permission to access" in row['content'] or "EEK!" in row['content'] or "Not Found" in row['content'] or len(row['content']) < 50 or "Are you a robot?" in row['content'] or "Access Denied:" in row['content'] or "502 Bad Gateway" in row['content']:
      index4.append(index)
    
whoisds.drop(index4 , inplace=True)
whoisds.drop_duplicates(inplace=True)

urldf = pd.DataFrame(columns= ['domain','hyphen', 'dot','digit', 'mimics','u_length','extension', 'entropy',
                                   'alexa_ngram_count','dict_ngram_count','squatting'])


for index, row in whoisds.iterrows():
    urldf.loc[len(urldf.index)] = [domain(row['url']), ct_hyphen(row['url']), ct_dot(row['url']), ct_digit(row['url']),
                                   mimics(row['url']),length(row['url']),
                                   extension(row['url']),entropy(row['url']),alexa_ngram_count(row['url']),dict_ngram_count(row['url']),
                                   squatting(row['url'])] 



contentdf = pd.DataFrame(columns= ['domain','intlink', 'extlink', '.js','a','meta','iframe','link', 'label', 'emptyhref',
                                   'link href','script','div','li','.php','h','img','href','p',
                                   'len','tags','suspicious_words','setAttribute']) 


for index, row in whoisds.iterrows():
    contentdf.loc[len(contentdf.index)] = [domain(row['url']), ct_intlink(row['content'],domain(row['url'])), ct_extlink(row['content'],domain(row['url'])), 
                                           ct_js(row['content']), ct_a(row['content']),ct_meta(row['content']),
                                           ct_iframe(row['content']),ct_link(row['content']),ct_label(row['content']),
                                           ct_empty_href(row['content']),ct_link_href(row['content']),ct_script(row['content']),ct_div(row['content']),
                                           ct_li(row['content']),ct_php(row['content']),ct_h(row['content']),ct_img(row['content']),
                                           ct_href(row['content']),ct_p(row['content']),length(row['content']),
                                           totaltags(row['content']),ct_susp_words(row['content']), ct_setAttribute(row['content'])] 


test_df = pd.merge(urldf, contentdf, left_index=True, right_index=True)
test_df = test_df.drop('domain_y',axis='columns')
test_df = test_df.rename(columns={'domain_x': 'domain'})     

test_df.isnull().values.any()  # No missing values

# Categorical : longer than 7 - squatting - phishing - mimics - u_susp words - extension

ord_enc = OrdinalEncoder()
test_df["squatting"] = ord_enc.fit_transform(test_df[["squatting"]])
test_df["mimics"] = ord_enc.fit_transform(test_df[["mimics"]])

# Removing extensions that are integer

print(len(test_df['extension'].unique()))

for index, row in test_df.iterrows():
  if row['extension'].isnumeric() :
    test_df['extension'][index] = '0'

test_df["extension"] = ord_enc.fit_transform(test_df[["extension"]])
print(len(test_df['extension'].unique()))

# object -> float conversion

domains = test_df['domain']
test_df = test_df.loc[:, test_df.columns != 'domain'].astype(float)

test_df['u_length_ext'] = test_df['u_length'].map(u_length2)
test_df['dot_ext'] = test_df['dot'].map(dot2)
test_df['setAttribute_ext'] = test_df['setAttribute'].map(setAttribute2)
test_df['hyphen_ext'] = test_df['hyphen'].map(hyphen2)

test_df

y_pred=classifier_rf.predict(test_df)

y_pred = pd.DataFrame(y_pred)  
import pandas as pd
domains = pd.DataFrame(domains)

domains1 = whoisds['url']

domains['y_pred'] = y_pred[0]

#y_pred.to_csv('y_pred.csv') 
domains.to_csv('/content/drive/MyDrive/ENS 491 Phishing detection/domains.csv')

domains

domains_pred = domains1.apply(domain).apply(domain)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd

domains = pd.read_csv('/content/drive/MyDrive/ENS 491 Phishing detection/domains.csv')
domains.drop(columns=domains.columns[0], axis=1,  inplace=True)

import pandas as pd

df1 = pd.read_csv('/content/drive/MyDrive/ENS 491 Phishing detection/virustotal1v3.csv')
df1.drop(columns=df1.columns[0], axis=1,  inplace=True)
df1

df2 = pd.read_csv('/content/drive/MyDrive/ENS 491 Phishing detection/virustotal2v3.csv')
df2.drop(columns=df2.columns[0], axis=1,  inplace=True)
df2

df3 = pd.read_csv('/content/drive/MyDrive/ENS 491 Phishing detection/virustotal3v3.csv')
df3.drop(columns=df3.columns[0], axis=1,  inplace=True)
df3

df4 = pd.read_csv('/content/drive/MyDrive/ENS 491 Phishing detection/virustotal4v3.csv')
df4.drop(columns=df4.columns[0], axis=1,  inplace=True)
df4

finalDF = pd.DataFrame(df1)
finalDF =finalDF.append(df2).reset_index(drop=True)
finalDF =finalDF.append(df3).reset_index(drop=True)
finalDF =finalDF.append(df4).reset_index(drop=True)
#finalDF['URL'] = finalDF['URL'].apply(domain).apply(domain)
finalDF

index_fin = finalDF[~finalDF['URL'].isin(domains_pred)].index
finalDF.drop(index_fin , inplace=True)

y_pred1 = pd.DataFrame(data=y_pred, index=None, columns=['y_pred'])
domains_pred1 = domains_pred.to_frame(name = 'domain_pred').reset_index()
pred_fin = domains_pred1.assign(y_pred=y_pred1)
pred_fin.drop(pred_fin.tail(1).index,inplace=True)

pred_fin.rename(columns = {'domain_pred':'URL'}, inplace = True)
pred_fin.drop_duplicates(subset='URL', keep='first', inplace=True)
finalDF.drop_duplicates(subset='URL', keep='first', inplace=True)

df_fin = pred_fin.copy()
df_fin = pd.merge(pred_fin, finalDF, on=["URL"] )

df_fin
df_fin['Status'] = df_fin['Status'].replace(['Clean'], 0)
df_fin['Status'] = df_fin['Status'].replace(['Malicious'], 1)
index = df_fin[df_fin['Status'] == "Not Found"].index
df_fin.drop(index , inplace=True)

df_fin['y_pred'] = domains['y_pred']
df_fin
df_fin['y_pred'] = df_fin['y_pred'].astype(int)

df_fin

print("Accuracy:",metrics.accuracy_score(df_fin['Status'], df_fin['y_pred']))

cm = confusion_matrix(df_fin['Status'], df_fin['y_pred'], labels=classifier_rf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=classifier_rf.classes_)
disp.plot(colorbar=False)
plt.show()

print(f1_score(df_fin['Status'], df_fin['y_pred'], average=None))
print(mean_squared_error(df_fin['Status'], df_fin['y_pred']))

# virustotalMaliciousDF = pd.DataFrame(df_fin)

# index_list =[]
# for index,row in df_fin.iterrows():
#   if(row['Status'] == 0):
#     index_list.append(index)
    
# virustotalMaliciousDF.drop(index_list, axis=0 , inplace=True)
# virustotalMaliciousDF = virustotalMaliciousDF.reset_index()
# del virustotalMaliciousDF['index']
# len(virustotalMaliciousDF)

# bothMaliciousDF = pd.DataFrame(virustotalMaliciousDF)

# index_list =[]
# for index,row in bothMaliciousDF.iterrows():
#   if(str(0) in str(row['y_pred'])):
#     index_list.append(index)
    
# bothMaliciousDF.drop(index_list, axis=0 , inplace=True)
# bothMaliciousDF = bothMaliciousDF.reset_index()
# del bothMaliciousDF['index']
# bothMaliciousDF

bothMaliciousDF =  pd.DataFrame(columns = ['index', 'URL', 'y_pred', 'Status', 'Total Detect', 'Total Engine'])

for index,row in df_fin.iterrows():
  if(row['Status'] == 0 and row['y_pred'] == 0):
    bothMaliciousDF.append(row)
    bothMaliciousDF.loc[len(bothMaliciousDF)] = row

bothMaliciousDF

"""Finding used technologies """

pip install webtech

#!/usr/bin/env python3
import webtech
import time 
# make sure to have the latest db version
webtech.database.update_database(force=True)

# you can use options, same as from the command line
wt = webtech.WebTech(options={'json': True})

whoisdsWebTechDF = pd.DataFrame().assign(URL=bothMaliciousDF['URL'])

site_list = whoisdsWebTechDF.values.tolist()

colmns = ['URL', 'Technologies']
datFrame = pd.DataFrame(columns = colmns)

# scan multiple websites from a list
for site in site_list:
  site = ("http://" + site[0]) #deitirr
  time.sleep(2)
  try:
    report = wt.start_from_url(site)
    techs = report['tech']

    tech_list = []
    #tech_name = str(tech_list)[1:-1] 
    for tech in techs:
      tech_name = list(tech.values())[0]
      tech_list.append(tech_name)
    datFrame = datFrame.append({'URL': site,'Technologies':tech_list},ignore_index=True)
    
  except webtech.utils.ConnectionException:
    datFrame = datFrame.append({'URL': site,'Technologies': "Unavailable" },ignore_index=True)


datFrame

"""Literature Review


*   https://www.researchgate.net/publication/327942719_Characterizing_Current_Features_of_Malicious_Threats_on_Websites 
1.   3.1.1 Features/Attributes of URLs -> (2) length of the host name, (4) number of dots (.), (5) number of specic symbol (hypens(-), underscores(_),for-warded slashes(/) equal sign(=)), (8) Suspicious words, (9) Domain features, (11) Country matching, Number of dots in domain
2.   3.2.1 Features/Attributes of JavaScript and HTML in Webpages -> Document length
*   https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359694
1. MOST PHISHING TARGETS
1 Paypal 
2 Amazon 
3 Microsoft 
4 Apple 
5 Facebook 
6 Google 
7 AOL News 
8 Internal Revenue Service 
9 US Automobile Association 
10 JPMorgan Chase and Co. 
2. url length, Many dot (.), Dash on the domain name, Top target 

*   file:///C:/Users/hp/Downloads/applsci-12-02806.pdf 
1. Length of JavaScript code *
2. Content length :  The total number of characters of the HTML content page.
3. Script tag references A count of the total number of SCRIPT tags in the HTML page content

*    https://towardsdatascience.com/extracting-feature-vectors-from-url-strings-for-malicious-url-detection-cbafc24737a
1. #script tags
2. length of html
3. #tags

*   https://hrcak.srce.hr/file/349850
*   https://dergipark.org.tr/tr/download/article-file/333655
Using @ symbol: Feature 4:It has been said that the previous part of "@" symbol in URL is
ignored by the browser.It has been said that the next part of "@" symbol in URL is often the real
address [44].

*   http://eprints.hud.ac.uk/id/eprint/24330/6/MohammadPhishing14July2015.pdf

https://github.com/SuperCowPowers/data_hacking/blob/master/dga_detection/DGA_Domain_Detection.ipynb

https://unit42.paloaltonetworks.com/cybersquatting/

https://raw.githubusercontent.com/dwyl/english-words/master/words.txt

https://github.com/elceef/dnstwist

https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/

https://mljar.com/blog/extract-rules-decision-tree/
"""
